{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyw7m/edD5uQT7H6bVE3zD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choycoy/final_project/blob/main/recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "cazuZ2w7V_yY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "q_df = pd.read_csv(\"/content/drive/My Drive/final/questions.csv\",sep=\",\")"
      ],
      "metadata": {
        "id": "3wV_GiYfPsL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__1.csv\",sep=\",\")\n",
        "df2 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__2.csv\",sep=\",\")\n",
        "df3 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__3.csv\",sep=\",\")\n",
        "df4 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__4.csv\",sep=\",\")\n",
        "df5 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__5.csv\",sep=\",\")\n",
        "df6 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__6.csv\",sep=\",\")\n",
        "df7 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__7.csv\",sep=\",\")\n",
        "df8 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__8.csv\",sep=\",\")\n",
        "df9 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__9.csv\",sep=\",\")\n",
        "df10 = pd.read_csv(\"/content/drive/My Drive/final/combined_with_filename__10.csv\",sep=\",\")\n",
        "\n",
        "\n",
        "df = pd.concat([df1, df2, df3, df4, df5, df6, df7, df8,df9, df10])"
      ],
      "metadata": {
        "id": "xMPC1iSPP2iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'question_id' column in df1 and df2 to the same data type (e.g., int64)\n",
        "df['question_id'] = df['question_id'].astype('int64')\n",
        "q_df['question_id'] = q_df['question_id'].astype('int64')\n",
        "\n",
        "# Merge the dataframes on 'question_id'\n",
        "merged_df = pd.merge(df, q_df[['question_id', 'correct_answer']], on='question_id', how='left')\n",
        "\n",
        "# Create the 'is_correct' column based on the comparison between 'user_answer' and 'correct_answer'\n",
        "merged_df['is_correct'] = (merged_df['user_answer'] == merged_df['correct_answer']).astype(int)\n",
        "\n",
        "# Now merged_df will have a new column 'is_correct' indicating if the user_answer is correct or not\n",
        "print(merged_df)\n"
      ],
      "metadata": {
        "id": "_C_eQLqfP5FG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming you have the dataframe df containing student interactions with columns: timestamp, solving_id, question_id, user_answer, elapsed_time, user_id, and is_correct (1 for correct, 0 for incorrect).\n",
        "\n",
        "# Sort the dataframe by timestamp to ensure the data is in chronological order\n",
        "merged_df.sort_values(by=['user_id', 'timestamp'], inplace=True)\n",
        "\n",
        "# Calculate Correctness Rate and Interaction Count for each user\n",
        "merged_df['correct_count'] = merged_df.groupby('user_id')['is_correct'].cumsum()  # Cumulative sum of correct answers for each user\n",
        "print(merged_df['correct_count'])\n",
        "\n",
        "merged_df['interaction_count'] = merged_df.groupby('user_id').cumcount() + 1  # Cumulative count of interactions for each user\n",
        "print(merged_df['interaction_count'])\n",
        "\n",
        "merged_df['correctness_rate'] = merged_df['correct_count'] / merged_df['interaction_count']  # Calculate the correctness rate for each interaction\n",
        "\n",
        "# Calculate average, minimum, and maximum elapsed time for each user\n",
        "merged_df['average_elapsed_time'] = merged_df.groupby('user_id')['elapsed_time'].transform('mean')\n",
        "\n",
        "merged_df"
      ],
      "metadata": {
        "id": "ISZ3D3adP5iE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_correctness = merged_df.groupby('user_id')['correctness_rate'].mean()\n",
        "user_correctness"
      ],
      "metadata": {
        "id": "KGUv7_wSP7OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_elapsed_time = merged_df.groupby('user_id')['average_elapsed_time'].mean()\n",
        "user_elapsed_time"
      ],
      "metadata": {
        "id": "H7exZR4BP-r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_metrics_df = pd.DataFrame({'user_id': user_correctness.index,\n",
        "                                'correctness_rate': user_correctness.values,\n",
        "                                'user_elapsed_time': user_elapsed_time.values})"
      ],
      "metadata": {
        "id": "HqHO9RqJQA_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_metrics_df['elapsed_time_scaled'] = (user_metrics_df['user_elapsed_time'] - user_metrics_df['user_elapsed_time'].min()) / (user_metrics_df['user_elapsed_time'].max() - user_metrics_df['user_elapsed_time'].min())\n",
        "user_metrics_df"
      ],
      "metadata": {
        "id": "ya1a6kQTQBc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "high_achievers_threshold = 0.8\n",
        "struggling_learners_threshold = 0.4\n",
        "\n",
        "def segment_users(row):\n",
        "    if row['correctness_rate'] >= high_achievers_threshold and row['elapsed_time_scaled'] <= 0.2:\n",
        "        return 'High Achievers'\n",
        "    elif row['correctness_rate'] < struggling_learners_threshold and row['elapsed_time_scaled'] >= 0.8:\n",
        "        return 'Struggling Learners'\n",
        "    else:\n",
        "        return 'Average Performers'\n",
        "\n",
        "user_metrics_df['user_segment'] = user_metrics_df.apply(segment_users, axis=1)"
      ],
      "metadata": {
        "id": "OkugyHyPQE07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install surprise"
      ],
      "metadata": {
        "id": "4iVHP4-FQFzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import Dataset, Reader\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "reader = Reader(rating_scale=(0, 1))\n",
        "custom_dataset = Dataset.load_from_df(merged_df[['user_id', 'question_id', 'correctness_rate']], reader)\n",
        "trainset, testset = train_test_split(custom_dataset, test_size=0.2)"
      ],
      "metadata": {
        "id": "nW1r8lDGQHq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from surprise import NMF\n",
        "\n",
        "# Choose the NMF algorithm\n",
        "algo = NMF()\n",
        "\n",
        "# Train the model on the training set\n",
        "algo.fit(trainset)\n",
        "\n",
        "# Predict ratings for the test set\n",
        "predictions = algo.test(testset)\n",
        "rmse = accuracy.rmse(predictions)\n",
        "mae = accuracy.mae(predictions)"
      ],
      "metadata": {
        "id": "_PwnlW9hQKV3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}